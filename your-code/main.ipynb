{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(url, tag, cls): return bs4(r.get(url).content, 'html.parser').find_all(tag, class_= cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\" \".join(re.sub(\"\\n\",\"\", i.text).split()) for i in get_tags(url, 'h1','h3 lh-condensed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\" \".join(re.sub(\"\\n\",\"\", i.text).split()).split('/')[1].split()[0] for i in get_tags(url, 'h1','h3 lh-condensed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['streamlit',\n",
       " 'DeepLearningExamples',\n",
       " 'Python-100-Days',\n",
       " 'whapa',\n",
       " 'tldr',\n",
       " 'machine_learning_examples',\n",
       " 'spleeter',\n",
       " 'tuya-convert',\n",
       " 'you-get',\n",
       " 'scikit-learn',\n",
       " 'searx',\n",
       " 'home-assistant',\n",
       " 'matplotlib',\n",
       " 'HASS-sonoff-ewelink',\n",
       " 'ipwndfu',\n",
       " 'UGATIT',\n",
       " 'urh',\n",
       " 'Osmedeus',\n",
       " 'Sublist3r',\n",
       " 'shadowsocks',\n",
       " 'MusicBot',\n",
       " 'linux-insides',\n",
       " 'keras-tuner',\n",
       " 'InstaPy',\n",
       " 'Shadowrocket-ADBlock-Rules']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generando URL completa de las imagenes\n",
    "l = [(\"https://en.wikipedia.org\" + i['href']) for i in get_tags(url, 'a','image')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/File:Walt_Disney_1946.JPG',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_1942_signature.svg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Trolley_Troubles_poster.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comprobando existencia de recurso\n",
    "l_status = [r.get(url) for url in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = r.get(url)\n",
    "cont = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4(cont, 'html.parser')\n",
    "l = soup.find_all('a', href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(\"https://en.wikipedia.org\" + i['href']) for i in l if i[\"href\"][0] != \"#\" if \"https\" not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.orghttps://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wikipedia.orghttps://en.wiktionary.org/wiki/python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=1',\n",
       " 'https://en.wikipedia.org/wiki/Pythonidae',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=2',\n",
       " 'https://en.wikipedia.org/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\" \".join(re.sub(\"\\n\",\"\",i.text).split()) for i in get_tags(url, 'div','usctitlechanged')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 6 - Domestic Security', 'Title 31 - Money and Finance ٭']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [re.sub(\"\\n\",\"\",i.text) for i in get_tags(url, 'h3','title')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID',\n",
       " 'JASON DEREK BROWN']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = r.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3].columns = df[3].columns.droplevel(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes_df = df[3].iloc[:,[3,4,6,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "      <th>Latitude degrees</th>\n",
       "      <th>Longitude degrees</th>\n",
       "      <th>Last update [-]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-03 19:09:12.51hr 00min ago</td>\n",
       "      <td>37.82</td>\n",
       "      <td>29.53</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-03 18:55:29.41hr 14min ago</td>\n",
       "      <td>40.47</td>\n",
       "      <td>20.81</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-03 18:55:11.11hr 14min ago</td>\n",
       "      <td>37.26</td>\n",
       "      <td>14.53</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-03 18:54:03.31hr 15min ago</td>\n",
       "      <td>35.67</td>\n",
       "      <td>117.53</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-03 18:51:33.61hr 18min ago</td>\n",
       "      <td>38.35</td>\n",
       "      <td>38.91</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-11-03 18:50:40.91hr 19min ago</td>\n",
       "      <td>35.56</td>\n",
       "      <td>117.40</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-11-03 18:42:07.01hr 27min ago</td>\n",
       "      <td>6.47</td>\n",
       "      <td>124.05</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-11-03 18:34:31.31hr 35min ago</td>\n",
       "      <td>37.61</td>\n",
       "      <td>36.41</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-11-03 18:30:16.61hr 39min ago</td>\n",
       "      <td>35.59</td>\n",
       "      <td>117.61</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-11-03 18:29:20.01hr 40min ago</td>\n",
       "      <td>17.83</td>\n",
       "      <td>69.81</td>\n",
       "      <td>SOUTHERN PERU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-11-03 18:23:19.81hr 46min ago</td>\n",
       "      <td>69.36</td>\n",
       "      <td>143.29</td>\n",
       "      <td>NORTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-11-03 18:16:42.01hr 53min ago</td>\n",
       "      <td>24.09</td>\n",
       "      <td>67.56</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-11-03 17:50:00.02hr 19min ago</td>\n",
       "      <td>18.29</td>\n",
       "      <td>105.32</td>\n",
       "      <td>OFF COAST OF JALISCO, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-11-03 17:26:15.12hr 43min ago</td>\n",
       "      <td>35.78</td>\n",
       "      <td>117.62</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-11-03 17:08:05.33hr 01min ago</td>\n",
       "      <td>35.63</td>\n",
       "      <td>117.46</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-11-03 17:02:03.83hr 07min ago</td>\n",
       "      <td>23.35</td>\n",
       "      <td>179.24</td>\n",
       "      <td>SOUTH OF FIJI ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-11-03 16:38:25.43hr 31min ago</td>\n",
       "      <td>14.71</td>\n",
       "      <td>171.71</td>\n",
       "      <td>VANUATU REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-11-03 16:35:38.93hr 34min ago</td>\n",
       "      <td>36.13</td>\n",
       "      <td>97.82</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-11-03 16:34:43.63hr 35min ago</td>\n",
       "      <td>38.35</td>\n",
       "      <td>38.93</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-11-03 16:31:05.83hr 38min ago</td>\n",
       "      <td>12.81</td>\n",
       "      <td>45.37</td>\n",
       "      <td>MAYOTTE REGION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date & Time UTC Latitude degrees Longitude degrees  \\\n",
       "0   2019-11-03 19:09:12.51hr 00min ago            37.82             29.53   \n",
       "1   2019-11-03 18:55:29.41hr 14min ago            40.47             20.81   \n",
       "2   2019-11-03 18:55:11.11hr 14min ago            37.26             14.53   \n",
       "3   2019-11-03 18:54:03.31hr 15min ago            35.67            117.53   \n",
       "4   2019-11-03 18:51:33.61hr 18min ago            38.35             38.91   \n",
       "5   2019-11-03 18:50:40.91hr 19min ago            35.56            117.40   \n",
       "6   2019-11-03 18:42:07.01hr 27min ago             6.47            124.05   \n",
       "7   2019-11-03 18:34:31.31hr 35min ago            37.61             36.41   \n",
       "8   2019-11-03 18:30:16.61hr 39min ago            35.59            117.61   \n",
       "9   2019-11-03 18:29:20.01hr 40min ago            17.83             69.81   \n",
       "10  2019-11-03 18:23:19.81hr 46min ago            69.36            143.29   \n",
       "11  2019-11-03 18:16:42.01hr 53min ago            24.09             67.56   \n",
       "12  2019-11-03 17:50:00.02hr 19min ago            18.29            105.32   \n",
       "13  2019-11-03 17:26:15.12hr 43min ago            35.78            117.62   \n",
       "14  2019-11-03 17:08:05.33hr 01min ago            35.63            117.46   \n",
       "15  2019-11-03 17:02:03.83hr 07min ago            23.35            179.24   \n",
       "16  2019-11-03 16:38:25.43hr 31min ago            14.71            171.71   \n",
       "17  2019-11-03 16:35:38.93hr 34min ago            36.13             97.82   \n",
       "18  2019-11-03 16:34:43.63hr 35min ago            38.35             38.93   \n",
       "19  2019-11-03 16:31:05.83hr 38min ago            12.81             45.37   \n",
       "\n",
       "                 Last update [-]  \n",
       "0                 WESTERN TURKEY  \n",
       "1                        ALBANIA  \n",
       "2                  SICILY, ITALY  \n",
       "3            SOUTHERN CALIFORNIA  \n",
       "4                 EASTERN TURKEY  \n",
       "5            SOUTHERN CALIFORNIA  \n",
       "6          MINDANAO, PHILIPPINES  \n",
       "7                 CENTRAL TURKEY  \n",
       "8            SOUTHERN CALIFORNIA  \n",
       "9                  SOUTHERN PERU  \n",
       "10               NORTHERN ALASKA  \n",
       "11            ANTOFAGASTA, CHILE  \n",
       "12  OFF COAST OF JALISCO, MEXICO  \n",
       "13           SOUTHERN CALIFORNIA  \n",
       "14           SOUTHERN CALIFORNIA  \n",
       "15         SOUTH OF FIJI ISLANDS  \n",
       "16                VANUATU REGION  \n",
       "17                      OKLAHOMA  \n",
       "18                EASTERN TURKEY  \n",
       "19                MAYOTTE REGION  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quakes_df.dropna()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, and title of upcoming hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = get_tags(url, 'a', 'link-box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_text = [l.text.replace('\\n', ' ').replace('\\xa0',',') for l in langs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' English 5,960,000+ articles ',\n",
       " ' Español 1,553,000+ artículos ',\n",
       " ' 日本語 1,174,000+ 記事 ',\n",
       " ' Deutsch 2,358,000+ Artikel ',\n",
       " ' Русский 1,575,000+ статей ',\n",
       " ' Français 2,149,000+ articles ',\n",
       " ' Italiano 1,561,000+ voci ',\n",
       " ' 中文 1,079,000+ 條目 ',\n",
       " ' Português 1,014,000+ artigos ',\n",
       " ' Polski 1,366,000+ haseł ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_tags(url, 'h2', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [dataset.text for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = r.get(url).content\n",
    "table = pd.read_html(cont)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Country of Origin</th>\n",
       "      <th>TotalCountries[a]</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>% of the World population (March 2019)[8]</th>\n",
       "      <th>Language familyBranch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandarin (language family)[9]</td>\n",
       "      <td>China</td>\n",
       "      <td>13</td>\n",
       "      <td>918.0</td>\n",
       "      <td>11.922</td>\n",
       "      <td>Sino-TibetanSinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>Spain</td>\n",
       "      <td>31</td>\n",
       "      <td>480.0</td>\n",
       "      <td>5.994</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>137</td>\n",
       "      <td>379.0</td>\n",
       "      <td>4.922</td>\n",
       "      <td>Indo-EuropeanGermanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi[10]</td>\n",
       "      <td>India</td>\n",
       "      <td>4</td>\n",
       "      <td>341.0</td>\n",
       "      <td>4.429</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>4</td>\n",
       "      <td>228.0</td>\n",
       "      <td>2.961</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>15</td>\n",
       "      <td>221.0</td>\n",
       "      <td>2.870</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russian</td>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>19</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Indo-EuropeanBalto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>Japan</td>\n",
       "      <td>2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.662</td>\n",
       "      <td>JaponicJapanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Western Punjabi[11]</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>2</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>India</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Language   Country of Origin  TotalCountries[a]  \\\n",
       "Rank                                                                         \n",
       "1     Mandarin (language family)[9]               China                 13   \n",
       "2                           Spanish               Spain                 31   \n",
       "3                           English      United Kingdom                137   \n",
       "4                         Hindi[10]               India                  4   \n",
       "5                           Bengali          Bangladesh                  4   \n",
       "6                        Portuguese            Portugal                 15   \n",
       "7                           Russian  Russian Federation                 19   \n",
       "8                          Japanese               Japan                  2   \n",
       "9               Western Punjabi[11]            Pakistan                  2   \n",
       "10                          Marathi               India                  1   \n",
       "\n",
       "      Speakers(millions)  % of the World population (March 2019)[8]  \\\n",
       "Rank                                                                  \n",
       "1                  918.0                                     11.922   \n",
       "2                  480.0                                      5.994   \n",
       "3                  379.0                                      4.922   \n",
       "4                  341.0                                      4.429   \n",
       "5                  228.0                                      2.961   \n",
       "6                  221.0                                      2.870   \n",
       "7                  154.0                                      2.000   \n",
       "8                  128.0                                      1.662   \n",
       "9                   92.7                                      1.204   \n",
       "10                  83.1                                      1.079   \n",
       "\n",
       "          Language familyBranch  \n",
       "Rank                             \n",
       "1           Sino-TibetanSinitic  \n",
       "2          Indo-EuropeanRomance  \n",
       "3         Indo-EuropeanGermanic  \n",
       "4       Indo-EuropeanIndo-Aryan  \n",
       "5       Indo-EuropeanIndo-Aryan  \n",
       "6          Indo-EuropeanRomance  \n",
       "7     Indo-EuropeanBalto-Slavic  \n",
       "8               JaponicJapanese  \n",
       "9       Indo-EuropeanIndo-Aryan  \n",
       "10      Indo-EuropeanIndo-Aryan  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[:10].set_index('Rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city:monterrey\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "resp = r.get(url)\n",
    "\n",
    "json_data = json.loads(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>clouds.all</th>\n",
       "      <th>cod</th>\n",
       "      <th>coord.lat</th>\n",
       "      <th>coord.lon</th>\n",
       "      <th>dt</th>\n",
       "      <th>id</th>\n",
       "      <th>main.humidity</th>\n",
       "      <th>main.pressure</th>\n",
       "      <th>main.temp</th>\n",
       "      <th>...</th>\n",
       "      <th>sys.country</th>\n",
       "      <th>sys.id</th>\n",
       "      <th>sys.sunrise</th>\n",
       "      <th>sys.sunset</th>\n",
       "      <th>sys.type</th>\n",
       "      <th>timezone</th>\n",
       "      <th>visibility</th>\n",
       "      <th>weather</th>\n",
       "      <th>wind.deg</th>\n",
       "      <th>wind.speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stations</td>\n",
       "      <td>75</td>\n",
       "      <td>200</td>\n",
       "      <td>25.67</td>\n",
       "      <td>-100.32</td>\n",
       "      <td>1572813260</td>\n",
       "      <td>3995465</td>\n",
       "      <td>52</td>\n",
       "      <td>1019</td>\n",
       "      <td>19.73</td>\n",
       "      <td>...</td>\n",
       "      <td>MX</td>\n",
       "      <td>7152</td>\n",
       "      <td>1572785440</td>\n",
       "      <td>1572825536</td>\n",
       "      <td>1</td>\n",
       "      <td>-21600</td>\n",
       "      <td>24140</td>\n",
       "      <td>[{'id': 803, 'main': 'Clouds', 'description': ...</td>\n",
       "      <td>110</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       base  clouds.all  cod  coord.lat  coord.lon          dt       id  \\\n",
       "0  stations          75  200      25.67    -100.32  1572813260  3995465   \n",
       "\n",
       "   main.humidity  main.pressure  main.temp  ...  sys.country  sys.id  \\\n",
       "0             52           1019      19.73  ...           MX    7152   \n",
       "\n",
       "  sys.sunrise  sys.sunset  sys.type  timezone  visibility  \\\n",
       "0  1572785440  1572825536         1    -21600       24140   \n",
       "\n",
       "                                             weather  wind.deg  wind.speed  \n",
       "0  [{'id': 803, 'main': 'Clouds', 'description': ...       110         6.2  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json_normalize(json_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_col = json.dumps(data.weather[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_col = json.loads(weather_col)\n",
    "weather_col = json_normalize(weather_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>icon</th>\n",
       "      <th>id</th>\n",
       "      <th>main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04d</td>\n",
       "      <td>803</td>\n",
       "      <td>Clouds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     description icon   id    main\n",
       "0  broken clouds  04d  803  Clouds"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
